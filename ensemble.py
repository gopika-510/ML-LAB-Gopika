# -*- coding: utf-8 -*-
"""Ensemble.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oA4TOuGaKa0HSXp8G3vrNhgrN-fgJmiH
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report
)

# Load dataset
data = load_breast_cancer()
df = pd.DataFrame(data.data, columns=data.feature_names)
df['target'] = data.target

# Preprocessing
X = StandardScaler().fit_transform(df.drop('target', axis=1))
y = df['target']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Different depths to test
depths = [2, 4, None]

# Evaluate for both criterions
for criterion in ["gini", "entropy"]:
    print("=" * 65)
    print(f" Evaluating Decision Tree Classifier (Criterion = {criterion})")
    print("=" * 65)

    for depth in depths:
        print(f"\n--- Testing max_depth = {depth} ---")
        model = DecisionTreeClassifier(
            criterion=criterion,
            max_depth=depth,
            min_samples_split=2,
            min_samples_leaf=1,
            random_state=42
        )

        # Train model
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1]

        # Calculate metrics
        acc = accuracy_score(y_test, y_pred)
        prec = precision_score(y_test, y_pred)
        rec = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_prob)

        print(f"Accuracy : {acc:.4f}")
        print(f"Precision: {prec:.4f}")
        print(f"Recall   : {rec:.4f}")
        print(f"F1 Score : {f1:.4f}")
        print(f"ROC-AUC  : {auc:.4f}")
        print("\nClassification Report:\n", classification_report(y_test, y_pred))

        # ROC Curve for each depth
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        plt.figure(figsize=(6, 4))
        plt.plot(fpr, tpr, label=f"AUC = {auc:.2f}")
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel("False Positive Rate")
        plt.ylabel("True Positive Rate")
        plt.title(f"ROC Curve ({criterion} | max_depth={depth})")
        plt.legend()
        plt.show()

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
# 1. Load Dataset
data = load_breast_cancer()
X = data.data
y = data.target
# Split train-test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Hyperparameter Tuning with GridSearchCV
params = {
    "n_estimators": [350, 400, 450],
    "learning_rate": [0.01, 0.1, 0.15]
}

grid = GridSearchCV(
    AdaBoostClassifier(random_state=42),
    params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid.fit(X_train, y_train)
best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)

# 3. 5-Fold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
fold = 1

for train_index, val_index in kf.split(X_train):
    X_tr, X_val = X_train[train_index], X_train[val_index]
    y_tr, y_val = y_train[train_index], y_train[val_index]

    best_model.fit(X_tr, y_tr)
    y_pred = best_model.predict(X_val)
    acc = accuracy_score(y_val, y_pred)
    print(f"Fold {fold} Accuracy: {acc:.4f}")
    fold_accuracies.append(acc)
    fold += 1

print("\nAverage CV Accuracy:", np.mean(fold_accuracies))

# 4. Final Evaluation on Test Set
y_pred_test = best_model.predict(X_test)
print("\n=== Performance Metrics on Test Set ===")
print("Accuracy :", accuracy_score(y_test, y_pred_test))
print("Precision:", precision_score(y_test, y_pred_test))
print("Recall   :", recall_score(y_test, y_pred_test))
print("F1 Score :", f1_score(y_test, y_pred_test))
print("ROC-AUC  :", roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))
print("\nClassification Report:\n", classification_report(y_test, y_pred_test))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report, roc_curve
)
# 1. Load Dataset
data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
# 2. Hyperparameter Tuning
params = {
    "n_estimators": [150, 100, 200],
    "max_depth": [3, 5, 23, 10],
    "min_samples_split": [2, 5, 10]
}

grid = GridSearchCV(
    RandomForestClassifier(random_state=42),
    params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid.fit(X_train, y_train)
best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)
# 3. 5-Fold Cross-Validation + ROC
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
plt.figure(figsize=(8, 6))

for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):
    X_tr, X_val = X_train[train_index], X_train[val_index]
    y_tr, y_val = y_train[train_index], y_train[val_index]

    best_model.fit(X_tr, y_tr)
    y_pred = best_model.predict(X_val)
    y_proba = best_model.predict_proba(X_val)[:, 1]
    acc = accuracy_score(y_val, y_pred)
    fold_accuracies.append(acc)
    print(f"Fold {fold} Accuracy: {acc:.4f}")

    fpr, tpr, _ = roc_curve(y_val, y_proba)
    auc = roc_auc_score(y_val, y_proba)
    plt.plot(fpr, tpr, label=f"Fold {fold} (AUC={auc:.2f})")

print("\nAverage CV Accuracy:", np.mean(fold_accuracies))
# 4. Final Test Evaluation
y_pred_test = best_model.predict(X_test)
y_proba_test = best_model.predict_proba(X_test)[:, 1]

print("\n=== Performance Metrics on Test Set ===")
print("Accuracy :", accuracy_score(y_test, y_pred_test))
print("Precision:", precision_score(y_test, y_pred_test))
print("Recall   :", recall_score(y_test, y_pred_test))
print("F1 Score :", f1_score(y_test, y_pred_test))
print("ROC-AUC  :", roc_auc_score(y_test, y_proba_test))
print("\nClassification Report:\n", classification_report(y_test, y_pred_test))

# -----------------------
# 5. Final ROC Curve for Test Set
# -----------------------
fpr_test, tpr_test, _ = roc_curve(y_test, y_proba_test)
auc_test = roc_auc_score(y_test, y_proba_test)

plt.plot(fpr_test, tpr_test, color="black", linewidth=2.5, label=f"Test ROC (AUC={auc_test:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest ROC Curve (5-Fold + Test)")
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report, roc_curve
)
# 1. Load Dataset
data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Hyperparameter Tuning
params = {
    "n_estimators": [50, 100, 350],
    "learning_rate": [0.3, 0.6, 0.9],
    "max_depth": [4, 5, 7]
}

grid = GridSearchCV(
    GradientBoostingClassifier(random_state=42),
    params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid.fit(X_train, y_train)
best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)
# 3. 5-Fold Cross-Validation + ROC
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
plt.figure(figsize=(8, 6))

for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):
    X_tr, X_val = X_train[train_index], X_train[val_index]
    y_tr, y_val = y_train[train_index], y_train[val_index]

    best_model.fit(X_tr, y_tr)
    y_pred = best_model.predict(X_val)
    y_proba = best_model.predict_proba(X_val)[:, 1]
    acc = accuracy_score(y_val, y_pred)
    fold_accuracies.append(acc)
    print(f"Fold {fold} Accuracy: {acc:.4f}")

    fpr, tpr, _ = roc_curve(y_val, y_proba)
    auc = roc_auc_score(y_val, y_proba)
    plt.plot(fpr, tpr, label=f"Fold {fold} (AUC={auc:.2f})")

print("\nAverage CV Accuracy:", np.mean(fold_accuracies))
# 4. Final Test Evaluation
y_pred_test = best_model.predict(X_test)
y_proba_test = best_model.predict_proba(X_test)[:, 1]

print("\n=== Performance Metrics on Test Set ===")
print("Accuracy :", accuracy_score(y_test, y_pred_test))
print("Precision:", precision_score(y_test, y_pred_test))
print("Recall   :", recall_score(y_test, y_pred_test))
print("F1 Score :", f1_score(y_test, y_pred_test))
print("ROC-AUC  :", roc_auc_score(y_test, y_proba_test))
print("\nClassification Report:\n", classification_report(y_test, y_pred_test))
# 5. Final ROC Curve for Test Set
fpr_test, tpr_test, _ = roc_curve(y_test, y_proba_test)
auc_test = roc_auc_score(y_test, y_proba_test)

plt.plot(fpr_test, tpr_test, color="black", linewidth=2.5, label=f"Test ROC (AUC={auc_test:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Gradient Boosting ROC Curve (5-Fold + Test)")
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from xgboost import XGBClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report, roc_curve
)
import warnings
warnings.filterwarnings("ignore")  # Suppress warnings globally

# 1. Load Dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Hyperparameter Tuning with Gamma Added
params = {
    "n_estimators": [500, 600, 650],
    "learning_rate": [0.01, 0.1, 0.2],
    "max_depth": [3, 5, 7],
    "gamma": [2, 4, 5]
}

grid = GridSearchCV(
    XGBClassifier(eval_metric="logloss", random_state=42),
    params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid.fit(X_train, y_train)
best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)

# 3. 5-Fold Cross-Validation + ROC Curve
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
plt.figure(figsize=(8, 6))

for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):
    X_tr, X_val = X_train[train_index], X_train[val_index]
    y_tr, y_val = y_train[train_index], y_train[val_index]

    best_model.fit(X_tr, y_tr)
    y_pred = best_model.predict(X_val)
    y_proba = best_model.predict_proba(X_val)[:, 1]
    acc = accuracy_score(y_val, y_pred)
    fold_accuracies.append(acc)
    print(f"Fold {fold} Accuracy: {acc:.4f}")

    fpr, tpr, _ = roc_curve(y_val, y_proba)
    auc = roc_auc_score(y_val, y_proba)
    plt.plot(fpr, tpr, label=f"Fold {fold} (AUC={auc:.2f})")

print("\nAverage CV Accuracy:", np.mean(fold_accuracies))

# 4. Final Evaluation on Test Set
y_pred_test = best_model.predict(X_test)
y_proba_test = best_model.predict_proba(X_test)[:, 1]

print("\n=== Performance Metrics on Test Set ===")
print("Accuracy :", accuracy_score(y_test, y_pred_test))
print("Precision:", precision_score(y_test, y_pred_test))
print("Recall   :", recall_score(y_test, y_pred_test))
print("F1 Score :", f1_score(y_test, y_pred_test))
print("ROC-AUC  :", roc_auc_score(y_test, y_proba_test))
print("\nClassification Report:\n", classification_report(y_test, y_pred_test))

# 5. Final ROC Curve for Test Set
fpr_test, tpr_test, _ = roc_curve(y_test, y_proba_test)
auc_test = roc_auc_score(y_test, y_proba_test)
plt.plot(fpr_test, tpr_test, color="black", linewidth=2.5, label=f"Test ROC (AUC={auc_test:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("XGBoost ROC Curve (5-Fold + Test)")
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import StackingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report, roc_curve
)
# 1. Load Dataset
data = load_breast_cancer()
X = data.data
y = data.target
# Split train-test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
# 2. Define Base Models and Stacking Classifier
base_estimators = [
    ('svm', SVC(probability=True, random_state=42)),
    ('nb', GaussianNB()),
    ('dt', DecisionTreeClassifier(random_state=42))
]
stack_model = StackingClassifier(
    estimators=base_estimators,
    final_estimator=LogisticRegression(max_iter=500, random_state=42),
    passthrough=False
)
# 3. Hyperparameter Tuning
params = {
    'svm__C': [0.1, 1, 10],
    'svm__kernel': ['linear', 'rbf'],
    'dt__max_depth': [3, 5, 7],
    'final_estimator__C': [0.1, 1, 10]
}

grid = GridSearchCV(
    stack_model,
    params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid.fit(X_train, y_train)
best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)
# 4. 5-Fold Cross-Validation + Fold-wise ROC
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
plt.figure(figsize=(8, 6))
for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):
    X_tr, X_val = X_train[train_index], X_train[val_index]
    y_tr, y_val = y_train[train_index], y_train[val_index]

    best_model.fit(X_tr, y_tr)
    y_pred = best_model.predict(X_val)
    y_proba = best_model.predict_proba(X_val)[:, 1]
    acc = accuracy_score(y_val, y_pred)
    fold_accuracies.append(acc)
    print(f"Fold {fold} Accuracy: {acc:.4f}")

    # ROC curve for each fold
    fpr, tpr, _ = roc_curve(y_val, y_proba)
    auc = roc_auc_score(y_val, y_proba)
    plt.plot(fpr, tpr, label=f"Fold {fold} (AUC = {auc:.2f})")

print("\nAverage CV Accuracy:", np.mean(fold_accuracies))
# 5. Final Evaluation on Test Set
y_pred_test = best_model.predict(X_test)
y_proba_test = best_model.predict_proba(X_test)[:, 1]

print("\n=== Performance Metrics on Test Set ===")
print("Accuracy :", accuracy_score(y_test, y_pred_test))
print("Precision:", precision_score(y_test, y_pred_test))
print("Recall   :", recall_score(y_test, y_pred_test))
print("F1 Score :", f1_score(y_test, y_pred_test))
print("ROC-AUC  :", roc_auc_score(y_test, y_proba_test))
print("\nClassification Report:\n", classification_report(y_test, y_pred_test))
# 6. Final ROC Curve for Test Set
fpr_test, tpr_test, _ = roc_curve(y_test, y_proba_test)
auc_test = roc_auc_score(y_test, y_proba_test)

plt.plot(fpr_test, tpr_test, color="black", linewidth=2.5, label=f"Test ROC (AUC = {auc_test:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Stacking Classifier ROC Curve (5-Fold + Test)")
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report, roc_curve
)

# 1. Load Dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split train-test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Define Base Models and Stacking Classifier
base_estimators = [
    ('lr1', LogisticRegression(max_iter=500, solver='liblinear', random_state=42)),
    ('lr2', LogisticRegression(max_iter=500, solver='liblinear', random_state=42)),
    ('lr3', LogisticRegression(max_iter=500, solver='liblinear', random_state=42))
]

stack_model = StackingClassifier(
    estimators=base_estimators,
    final_estimator=LogisticRegression(max_iter=500, solver='liblinear', random_state=42),
    passthrough=False
)

# 3. Hyperparameter Tuning for Logistic Regression
params = {
    'lr1__C': [0.01, 0.1, 1, 10],
    'lr2__C': [0.01, 0.1, 1, 10],
    'lr3__C': [0.01, 0.1, 1, 10],
    'final_estimator__C': [0.01, 0.1, 1, 10]
}

grid = GridSearchCV(
    stack_model,
    params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid.fit(X_train, y_train)
best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)

# 4. 5-Fold Cross-Validation + ROC Curve
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
plt.figure(figsize=(8, 6))

for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):
    X_tr, X_val = X_train[train_index], X_train[val_index]
    y_tr, y_val = y_train[train_index], y_train[val_index]

    best_model.fit(X_tr, y_tr)
    y_pred = best_model.predict(X_val)
    y_proba = best_model.predict_proba(X_val)[:, 1]
    acc = accuracy_score(y_val, y_pred)
    fold_accuracies.append(acc)
    print(f"Fold {fold} Accuracy: {acc:.4f}")

    # ROC curve for each fold
    fpr, tpr, _ = roc_curve(y_val, y_proba)
    auc = roc_auc_score(y_val, y_proba)
    plt.plot(fpr, tpr, label=f"Fold {fold} (AUC = {auc:.2f})")

print("\nAverage CV Accuracy:", np.mean(fold_accuracies))

# 5. Final Evaluation on Test Set
y_pred_test = best_model.predict(X_test)
y_proba_test = best_model.predict_proba(X_test)[:, 1]

print("\n=== Performance Metrics on Test Set ===")
print("Accuracy :", accuracy_score(y_test, y_pred_test))
print("Precision:", precision_score(y_test, y_pred_test))
print("Recall   :", recall_score(y_test, y_pred_test))
print("F1 Score :", f1_score(y_test, y_pred_test))
print("ROC-AUC  :", roc_auc_score(y_test, y_proba_test))
print("\nClassification Report:\n", classification_report(y_test, y_pred_test))

# 6. Final ROC Curve for Test Set
fpr_test, tpr_test, _ = roc_curve(y_test, y_proba_test)
auc_test = roc_auc_score(y_test, y_proba_test)
plt.plot(fpr_test, tpr_test, color="black", linewidth=2.5, label=f"Test ROC (AUC = {auc_test:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Stacking Classifier with Logistic Regression ROC Curve (5-Fold + Test)")
plt.legend()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.ensemble import StackingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, classification_report, roc_curve
)

# 1. Load Dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 2. Define Base Models and Stacking Classifier
base_estimators = [
    ('svm', SVC(probability=True, random_state=42)),
    ('knn', KNeighborsClassifier()),
    ('dt', DecisionTreeClassifier(random_state=42))
]

stack_model = StackingClassifier(
    estimators=base_estimators,
    final_estimator=LogisticRegression(max_iter=500, random_state=42),
    passthrough=False
)

# 3. Hyperparameter Tuning
params = {
    'svm__C': [0.1, 1, 10],
    'svm__kernel': ['linear', 'rbf'],
    'knn__n_neighbors': [3, 5, 7],
    'knn__weights': ['uniform', 'distance'],
    'dt__max_depth': [3, 5, 7],
    'final_estimator__C': [0.1, 1, 10]
}

grid = GridSearchCV(
    stack_model,
    params,
    cv=5,
    scoring="accuracy",
    n_jobs=-1
)

grid.fit(X_train, y_train)
best_model = grid.best_estimator_
print("Best Parameters:", grid.best_params_)

# 4. 5-Fold Cross-Validation + Fold-wise ROC
kf = KFold(n_splits=5, shuffle=True, random_state=42)
fold_accuracies = []
plt.figure(figsize=(8, 6))

for fold, (train_index, val_index) in enumerate(kf.split(X_train), start=1):
    X_tr, X_val = X_train[train_index], X_train[val_index]
    y_tr, y_val = y_train[train_index], y_train[val_index]

    best_model.fit(X_tr, y_tr)
    y_pred = best_model.predict(X_val)
    y_proba = best_model.predict_proba(X_val)[:, 1]
    acc = accuracy_score(y_val, y_pred)
    fold_accuracies.append(acc)
    print(f"Fold {fold} Accuracy: {acc:.4f}")

    # ROC curve for each fold
    fpr, tpr, _ = roc_curve(y_val, y_proba)
    auc = roc_auc_score(y_val, y_proba)
    plt.plot(fpr, tpr, label=f"Fold {fold} (AUC = {auc:.2f})")

print("\nAverage CV Accuracy:", np.mean(fold_accuracies))

# 5. Final Evaluation on Test Set
y_pred_test = best_model.predict(X_test)
y_proba_test = best_model.predict_proba(X_test)[:, 1]

print("\n=== Performance Metrics on Test Set ===")
print("Accuracy :", accuracy_score(y_test, y_pred_test))
print("Precision:", precision_score(y_test, y_pred_test))
print("Recall   :", recall_score(y_test, y_pred_test))
print("F1 Score :", f1_score(y_test, y_pred_test))
print("ROC-AUC  :", roc_auc_score(y_test, y_proba_test))
print("\nClassification Report:\n", classification_report(y_test, y_pred_test))

# 6. Final ROC Curve for Test Set
fpr_test, tpr_test, _ = roc_curve(y_test, y_proba_test)
auc_test = roc_auc_score(y_test, y_proba_test)

plt.plot(fpr_test, tpr_test, color="black", linewidth=2.5, label=f"Test ROC (AUC = {auc_test:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Stacking Classifier (SVM + KNN + DT) ROC Curve (5-Fold + Test)")
plt.legend()
plt.show()